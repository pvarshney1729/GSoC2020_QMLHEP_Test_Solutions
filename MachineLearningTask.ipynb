{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearningTask.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjxxyxasYZRL",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning Part\n",
        "\n",
        "You will explore how best to apply machine learning algorithms, for example Neural Network, Boosted Decision Tree (BDT), Support Vector Machine(SVM) to solve a High Energy Data analysis issue, more specifically,  separating the signal events from the background events.\n",
        "\n",
        "A set of input samples (simulated with Delphes) is provided in NumPy NPZ format [Download Input](https://drive.google.com/open?id=1r_MZB_crfpij6r3SxPDeU_3JD6t6AxAj). In the input file, there are only 100 samples for training and 100 samples for testing so it won’t take much computing resources to accomplish this task. The signal events are labeled with 1 while the background sample are labeled with 0.\n",
        "\n",
        "You can apply one machine learning algorithm to this input but be sure to show that you understand how to fine tune your machine learning model to improve the performance. The performance can be evaluated with classification accuracy or Area Under ROC Curve (AUC).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmix0pCwbAYQ",
        "colab_type": "text"
      },
      "source": [
        "##Loading and Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kysewghsuaHE",
        "colab_type": "code",
        "outputId": "1578f04e-4c7f-4755-e685-7808b97b9861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "## Mount Drive into Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !cd drive/My\\ Drive/ \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQPUdOZoy8W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "file = np.load('drive/My Drive/QIS_EXAM_200Events.npz', allow_pickle=True)\n",
        "\n",
        "# temp = file['training_input']\n",
        "# print(temp.item())\n",
        "\n",
        "train_dataset = file['training_input'].item()\n",
        "test_dataset = file['test_input'].item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5eMRTb8zTQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_train_0 = pd.DataFrame(train_dataset['0'])\n",
        "df_test_0  = pd.DataFrame(test_dataset['0'])\n",
        "\n",
        "df_train_1 = pd.DataFrame(train_dataset['1'])\n",
        "df_test_1  = pd.DataFrame(test_dataset['1'])\n",
        "\n",
        "df_train = df_train_0.append(df_train_1)\n",
        "df_test  = df_test_0.append(df_test_1)\n",
        "\n",
        "df_train_0 -= df_train.mean()\n",
        "df_train_1 -= df_train.mean()\n",
        "\n",
        "df_test_0 -= df_test.mean()\n",
        "df_test_1 -= df_test.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3bncUZO1d6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train = np.append(df_train_0.values, df_train_1.values, axis = 0)\n",
        "y_train = np.append(np.zeros(shape = (1,50)), np.ones(shape = (1,50)))\n",
        "\n",
        "X_test = np.append(df_test_0.values, df_test_1.values, axis = 0)\n",
        "y_test = np.copy(y_train)\n",
        "\n",
        "X_train, y_train = shuffle(X_train, y_train)\n",
        "X_test,  y_test  = shuffle(X_test,  y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PYagBlobLqK",
        "colab_type": "text"
      },
      "source": [
        "##ML Algorithms\n",
        "###1. Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD1Q1JQ2DvOE",
        "colab_type": "code",
        "outputId": "76dca322-8ad9-4b21-c6e2-4e279388fc71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "#Quoting from sklearn documentations \"For small datasets, ‘liblinear’ is a good choice\"\n",
        "classifier = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
        "logistic_regression = classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "# print(y_pred)\n",
        "\n",
        "auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "print('AUC: %.5f' % auc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
            " 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1.\n",
            " 0. 0. 0. 1.]\n",
            "AUC: 0.72000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrqzPQ7UbU7J",
        "colab_type": "text"
      },
      "source": [
        "###2. Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br3f7trfE3q1",
        "colab_type": "code",
        "outputId": "76343a3a-b8de-4031-d9e2-2987c5dd75d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#Any value of max_depth other than 2 or 3 significantly decreases the accuracy.\n",
        "#Entropy seems to perform better almost always than the gini criteria\n",
        "decision_tree =  DecisionTreeClassifier(criterion = \"entropy\", random_state = 32, max_depth = 3, min_samples_split = 3)\n",
        "\n",
        "decision_tree.fit(X_train,y_train)\n",
        "y_pred_dt = decision_tree.predict(X_test)\n",
        "\n",
        "# print(y_pred_dt)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_dt)\n",
        "print('AUC: %.5f' % auc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.72000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEnRQzK9bkay",
        "colab_type": "text"
      },
      "source": [
        "###3. Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh9uOUTmF4lZ",
        "colab_type": "code",
        "outputId": "3a64802c-c09a-45a9-efc2-dfcb524424bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Any value of max_depth other than 2 or 3 significantly decreases the accuracy.\n",
        "#Entropy seems to perform better almost always than the gini criteria\n",
        "#The performance of the random forest increases with no of estimators(#trees) and peaks at around 100 trees\n",
        "random_forest_classifier = RandomForestClassifier(criterion = \"entropy\", max_depth = 2, random_state = 32, n_estimators = 100, min_samples_split = 3)\n",
        "\n",
        "scores = cross_val_score(random_forest_classifier, X_train, y_train, cv=10)\n",
        "print(\"Cross Validation Average Accuracy: {:.5f}\".format(scores.mean()))\n",
        "\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "score = random_forest_classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Random Forest Test Set Accuracy: {:.5f}\".format(score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation Average Accuracy: 0.80000\n",
            "Random Forest Test Set Accuracy: 0.72000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGMgR-jEbngG",
        "colab_type": "text"
      },
      "source": [
        "###4. Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjujW6TuC6NG",
        "colab_type": "code",
        "outputId": "caf83596-7f5d-4c67-953e-0a09ee54d369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Default rbf and poly kernels work a bit better than linear kernels\n",
        "svm_classifier = svm.SVC(kernel = 'poly', break_ties = 'true', random_state = 32)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"SVM Test Set Accuracy: {:.5f}\".format(svm_classifier.score(X_test, y_test)))\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "print(\"SVM Prediction Report: \\n {}\".format(classification_report(y_test, y_pred)))\n",
        "\n",
        "#Value of C in 1- to 1e1 has the same accuracy\n",
        "svm_classifier = svm.SVC(C = 1e1, kernel = 'poly', break_ties = 'true', random_state = 32)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"SVM Test Set Accuracy: {:.5f}\".format(svm_classifier.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Test Set Accuracy: 0.72000\n",
            "SVM Prediction Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.82      0.75        50\n",
            "         1.0       0.78      0.62      0.69        50\n",
            "\n",
            "    accuracy                           0.72       100\n",
            "   macro avg       0.73      0.72      0.72       100\n",
            "weighted avg       0.73      0.72      0.72       100\n",
            "\n",
            "SVM Test Set Accuracy: 0.72000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYw7_Uw2bzOb",
        "colab_type": "text"
      },
      "source": [
        "###5. K Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtQVJ7FKGtbs",
        "colab_type": "code",
        "outputId": "2000897a-781e-45ae-d502-af453bb101bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#Any other cluster amount reduces accuracy\n",
        "#Also note, #clusters should be clearly <= 4\n",
        "num_clusters = 3\n",
        "\n",
        "kmeans = KMeans(n_clusters = num_clusters, random_state = 32)\n",
        "clusters_train = kmeans.fit_predict(X_train)\n",
        "clusters_test = kmeans.predict(X_test)\n",
        "\n",
        "score_all = 0\n",
        "for i in range(0, num_clusters):\n",
        "    svm_classifier = svm.SVC(kernel = 'poly', degree = 3, break_ties = 'true', random_state = 32)\n",
        "    svm_classifier.fit(X_train[clusters_train == i], y_train[clusters_train == i])\n",
        "\n",
        "    score = svm_classifier.score(X_test[clusters_test == i], y_test[clusters_test == i])\n",
        "    print(\"SVM accuracy for class {}: {:.5f}\".format(i, score))\n",
        "    \n",
        "    score_all += score\n",
        "\n",
        "print(\"K means classification accuracy: {:.5f}\".format(score_all / num_clusters))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM accuracy for class 0: 0.75000\n",
            "SVM accuracy for class 1: 0.66038\n",
            "SVM accuracy for class 2: 0.81481\n",
            "K means classification accuracy: 0.74173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSi6zMOscDIM",
        "colab_type": "text"
      },
      "source": [
        "###6. Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw-jjS55GxTw",
        "colab_type": "code",
        "outputId": "71c70152-3176-4b2a-b21c-8dbacd2854cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "naive_bayes_classifier = GaussianNB()\n",
        "naive_bayes_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred =  naive_bayes_classifier.predict(X_test)\n",
        "\n",
        "auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "print('AUC: %.5f' % auc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.71000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRZwUuuKcKgL",
        "colab_type": "text"
      },
      "source": [
        "###7. Boosted Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asg1J0u-G9wE",
        "colab_type": "code",
        "outputId": "91d122b4-a146-42ab-8897-da433f1cc94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "#The Cross validation accuracy seems to decrease with increase in number of estimators. Huh!?\n",
        "boosted_random_forest = AdaBoostClassifier(random_forest_classifier, n_estimators = 5, random_state = 32)\n",
        "scores = cross_val_score(boosted_random_forest, X_train, y_train, cv=10)\n",
        "print(\"Cross Validation Average Accuracy: {:.5f}\".format(scores.mean()))\n",
        "\n",
        "boosted_random_forest.fit(X_train, y_train)\n",
        "score = boosted_random_forest.score(X_test, y_test)\n",
        "print(\"Boosted Random Forest Test Set Accuracy: {:.5f}\".format(score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Validation Average Accuracy: 0.78000\n",
            "Boosted Random Forest Test Set Accuracy: 0.61000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQA3TiNvcZHT",
        "colab_type": "text"
      },
      "source": [
        "###8. Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOUc5ke-HWR8",
        "colab_type": "code",
        "outputId": "926a6094-11f0-4b49-a0a0-d6e6cde0901a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Stochastic hyperparameter search\n",
        "count = 10\n",
        "max_score = 0\n",
        "\n",
        "while (count > 0):\n",
        "  count = count - 1\n",
        "  \n",
        "  model = tf.keras.models.Sequential([\n",
        "    keras.layers.Dense(64, input_dim = 5, activation = 'relu', kernel_initializer='random_uniform', kernel_regularizer = keras.regularizers.l2(0.01)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(16,  activation = 'relu', kernel_initializer='random_uniform', kernel_regularizer = keras.regularizers.l2(0.01)),\n",
        "    keras.layers.Dropout(.2),\n",
        "    keras.layers.Dense(4,  activation = 'relu', kernel_initializer='random_uniform', kernel_regularizer = keras.regularizers.l2(0.01)),\n",
        "    keras.layers.Dropout(.2),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X_train, y_train, epochs=20, batch_size = 20, verbose=0)\n",
        "\n",
        "  test_loss, score = model.evaluate(X_test, y_test)\n",
        "  print('Test accuracy:', score)\n",
        "  \n",
        "  if score > max_score:\n",
        "    max_score = score\n",
        "    model.save('my_model')\n",
        "\n",
        "\n",
        "\n",
        "loaded_model = keras.models.load_model('my_model')\n",
        "accuracy = loaded_model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "print(\"Neural Network Test Data Accuracy: {}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 2s 20ms/sample - loss: 0.6905 - acc: 0.5000\n",
            "Test accuracy: 0.5\n",
            "100/100 [==============================] - 2s 21ms/sample - loss: 0.6839 - acc: 0.6700\n",
            "Test accuracy: 0.67\n",
            "100/100 [==============================] - 2s 21ms/sample - loss: 0.6837 - acc: 0.7100\n",
            "Test accuracy: 0.71\n",
            "100/100 [==============================] - 2s 21ms/sample - loss: 0.6903 - acc: 0.7100\n",
            "Test accuracy: 0.71\n",
            "100/100 [==============================] - 2s 21ms/sample - loss: 0.6913 - acc: 0.7300\n",
            "Test accuracy: 0.73\n",
            "100/100 [==============================] - 2s 21ms/sample - loss: 0.6855 - acc: 0.7300\n",
            "Test accuracy: 0.73\n",
            "100/100 [==============================] - 2s 22ms/sample - loss: 0.6902 - acc: 0.6500\n",
            "Test accuracy: 0.65\n",
            "100/100 [==============================] - 2s 22ms/sample - loss: 0.6838 - acc: 0.6900\n",
            "Test accuracy: 0.69\n",
            "100/100 [==============================] - 2s 22ms/sample - loss: 0.6926 - acc: 0.6800\n",
            "Test accuracy: 0.68\n",
            "100/100 [==============================] - 2s 23ms/sample - loss: 0.6862 - acc: 0.7000\n",
            "Test accuracy: 0.7\n",
            "Neural Network Test Data Accuracy: 0.7300000190734863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCzjCunnIH03",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Analysis and Conclusions.\n",
        "\n",
        "I have compared various Machine Learning classification Algorithms using AUC as a metric.\n",
        "\n",
        "*   I have mean centered the data (normalisation), however it cannot be attributed to impacting the AUC value much (See last bullet).\n",
        "\n",
        "*  The final scores for the various Machine Learning Algorithms implemented are as follows:-\n",
        "\n",
        "  1.   Logistic Regression : 0.72\n",
        "  2.   Decision Tree : 0.72\n",
        "  3.   Random Forest : 0.720\n",
        "  4.   SVM Classifier : 0.720\n",
        "  5.   K Means + SVM Classifier : 0.74173\n",
        "  6.   Naive Bayes' Classifier : 0.71\n",
        "  7.   AdaBoost - 0.610\n",
        "  8.   Neural Network : 0.7300\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   The best results were obtained by K Means + SVM Algorithm\n",
        "\n",
        "*   Since machine learning algorithms are data intensive; Convergence cannot be guaranteed without sufficient data along with a significant variation in accuracy. This is also evident in our observations above"
      ]
    }
  ]
}